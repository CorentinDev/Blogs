---
ID: 197
post_title: 'Hadoop, part 2 : the link between Hadoop and BIRT'
author: edaboussi
post_date: 2012-08-07 14:54:00
post_excerpt: |
  <p>In this second blog post concerning Hadoop, we are going to focus on the link between Hadoop and BIRT, the Eclipse-based reporting system. We will first present Hive, a component of Hadoop's ecosystem, then we will see how it can be connected to BIRT, and we will finish with an example that will illustrate more concretely how things work. If you missed the first episode, which is an introduction to Hadoop, its file system, and MapReduce, you can follow this <a href="/index.php?post/2012/07/11/Hadoop-et-le-MapReduce-au-service-des-gros-volumes-de-donn%C3%A9es" hreflang="fr">link</a>, to read the first part of this Hadoop blog post series.</p>
layout: post
permalink: http://blog.zenika-offres.com/?p=197
published: true
---
<p>In this second blog post concerning Hadoop, we are going to focus on the link between Hadoop and BIRT, the Eclipse-based reporting system. We will first present Hive, a component of Hadoop's ecosystem, then we will see how it can be connected to BIRT, and we will finish with an example that will illustrate more concretely how things work. If you missed the first episode, which is an introduction to Hadoop, its file system, and MapReduce, you can follow this <a href="/index.php?post/2012/07/11/Hadoop-et-le-MapReduce-au-service-des-gros-volumes-de-donn%C3%A9es" hreflang="fr">link</a>, to read the first part of this Hadoop blog post series.</p>
<!--more-->
<h3>1. Introduction to Hive&nbsp;:</h3> <p>Hive is an Hadoop sub-project which lets you store data and make queries with a SQL-like language&nbsp;: HQL (Hive Query Language). From data (stored on the HDFS, but also local data), we can create tables, through Hive, that will be stored on the HDFS (this underlines the "data warehouse" role of Hive), and then make HQL queries. One of the fundamental interest of Hive is MapReduce&nbsp;: Hive is part of the Hadoop ecosystem, hence it uses MapReduce for every HQL query. Some queries will not require any Reducing task, but in every case this is suited for distributed applications. Furthermore, Hive also lets you create your own HQL queries, as long as you provide the corresponding Mappers and Reducers, in a similar way as what has been done in the previous part of this blog series. Here is a simplified scheme of the way Hive is integrated to Hadoop&nbsp;:</p> <p><img src="/wp-content/uploads/2015/07/.Hive4_m.jpg" alt="hive4" style="display:block; margin:0 auto;" title="hive4" /></p> <h3>2. The link between Hive and BIRT&nbsp;:</h3> <p>BIRT ("Business Intelligence and Reporting Tools") is the Eclipse project for reporting. Since its version 3.7.2, it is possible to link it to Hive, which means that we now can make HQL queries directly from BIRT, on tables stored on the HDFS. (see <a href="http://www.eclipse.org/birt/phoenix/project/notable3.7.php#jump_4">here</a> for the official release statement).</p> <p>Thus, we do not make the query from Hive anymore, but from BIRT&nbsp;: then we can do operations/treatments on the imported data, and in the end export the final report in the BIRT supported file formats (.pdf, .html, .doc, etc….) The connection uses JDBC and the hiveserver, defined on port 10000 by default, as we can see on this expanded scheme&nbsp;:</p> <p><img src="/wp-content/uploads/2015/07/.Hive_birt4_m.jpg" alt="hivebirt4" style="display:block; margin:0 auto;" title="hivebirt4" /></p> <h3>3. A use case&nbsp;: wikipedia dumps&nbsp;:</h3> <p>To illustrate the way this BIRT connector works, we are going to use the <a href="http://dumps.wikimedia.org/enwiki/">dumps</a> provided by Wikipedia. We decided to download the complete list of the titles of wikipedia.com articles on the american website (a 200MB text file, we will call it "wikidata.txt"), in order to treat this data with BIRT, and then export it as a report.</p> <p>Let's detail the different stages of the process&nbsp;:</p> <p>First, we have to put the data file on the HDFS, in order to create the data tables afterwards&nbsp;:</p> <p><code>hadoop fs -put /local_directory/wikidata.txt /hdfs_directory</code></p> <p>Then, we can launch Hive, create the table and load it with the wikipedia dump&nbsp;:</p> <p><code>#hive</code></p> <p><code>hive&gt; CREATE TABLE donnees (nom STRING);</code></p> <p><code>hive&gt; LOAD DATA INPATH 'hdfs_directory/wikidata.txt' INTO TABLE donnees;</code></p> <p>Thus we can start the Hive server&nbsp;:</p> <p><code>#hive --service hiveserver</code></p> <p>We can now connect to the Hive server through JDBC, by creating a new BIRT data source, and as this example is un in local, the URL is&nbsp;: jdbc:hive://localhost:10000/default</p> <p><img src="/wp-content/uploads/2015/07/jdbc.png" alt="jdbc" style="display:block; margin:0 auto;" title="jdbc" /></p> <p>We now need to create a new data set, this is done using a HQL query&nbsp;: for example we can look for all the articles containing "Kennedy" in our table&nbsp;:</p> <p><img src="/wp-content/uploads/2015/07/hql.png" alt="hql" style="display:block; margin:0 auto;" title="hql" /></p> <p>BIRT can then be used as usual to make all the required treatments/aggregations,etc… During the previewing or exporting tasks, we can see the MapReduce tasks in real time with the Jobtracker, provided by Hadoop&nbsp;:</p> <p><img src="/wp-content/uploads/2015/07/.jobtracker_m.jpg" alt="job" style="display:block; margin:0 auto;" title="job" /></p> <p>When the process is over, the report is ready to be published, in one of the BIRT supported file formats.</p> <h3>4. Conclusion&nbsp;:</h3> <p>In a nutshell, we have been able to establish the connection between hive and BIRT, and we can see two major advantages&nbsp;:</p> <p>- This connector is very useful in the import process&nbsp;: all we have to do is to connect to Hive, everything else is done directly from BIRT, as long as the tables have already been created.</p> <p>- The MapReduce implementation, which follows the Hadoop</p> <p>However, some limitations remain&nbsp;:</p> <p>- Some operations, like aggregation, are processed through BIRT but do not use the MapReduce&nbsp;: the best solution, if you want to really exploit the distributed processing, is to reduce the data through a precise MapReduce HQL query, then process it through BIRT.</p> <p>- The tables have to be created before the BIRT importing process, (with Hive).</p> <p>I hope that you could see the interest of this feature, and I look forward to presenting you another Hadoop-related article, for the next post of this series.</p>
---
ID: 150
post_title: Spark et Cassandra
author: Gérald Quintana
post_date: 2015-02-23 16:00:00
post_excerpt: |
  <p><img src="/public/spark-cassandra/spark-cassandra2.png" alt="Spark &amp; Cassandra" style="float:right; margin: 0 0 1em 1em;" title="Spark &amp; Cassandra" /> <a href="http://cassandra.apache.org">Cassandra</a> est une base de données distribuée capable de stocker de gros volumes de données. Si son modèle de données constitué de tables et de colonnes, et son langage de requêtage CQL imitent fortement les bases de données relationnelles, la ressemblance s'arrête là. Les possibilités de requêtage dépendent intrinsèquement de la manière dont sont stockées/modélisées les données.</p>
layout: post
permalink: http://blog.zenika-offres.com/?p=150
published: true
slide_template:
  - default
---
<a href="http://cassandra.apache.org">Cassandra</a> est une base de données distribuée capable de stocker de gros volumes de données. Si son modèle de données constitué de tables et de colonnes, et son langage de requêtage CQL imitent fortement les bases de données relationnelles, la ressemblance s'arrête là. Les possibilités de requêtage dépendent intrinsèquement de la manière dont sont stockées/modélisées les données.

<!--more-->

Datastax fournit un <a href="https://github.com/datastax/spark-cassandra-connector">connecteur Spark</a> qui permet manipuler des tables Cassandra sous forme de RDD, concept que nous avons présenté lors du précédent article d'<a href="http://blog.zenika.com/index.php?post/2015/02/02/Introduction-a-Spark">introduction à Spark</a>. Par le biais de ce connecteur, Spark apporte à Cassandra des capacités de requêtage analytique.

D'une manière plus générale, nous étudierons l'intégration de Cassandra et Spark. Ce sera aussi l'occasion de découvrir les possibilités de Spark SQL et Spark Streaming.
<h3>Spark classique</h3>
Pour commencer avec le connecteur Cassandra Spark, les Jobs Spark doivent pouvoir se connecter à Cassandra. Pour cela on place dans la configuration les identifiants de connexion à Cassandra:
<pre class="java code java" style="font-family: inherit;">SparkConf conf <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> SparkConf<span style="color: #009900;">(</span><span style="color: #000066; font-weight: bold;">true</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">setMaster</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"local"</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">setAppName</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"Zenika"</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">set</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"spark.cassandra.connection.host"</span>, <span style="color: #0000ff;">"node1"</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">set</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"spark.cassandra.connection.username"</span>, <span style="color: #0000ff;">"spark"</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">set</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"spark.cassandra.connection.password"</span>, <span style="color: #0000ff;">"spark"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Pour lire les données dans Cassandra, on décrit au moyen de la classe <code>CassandraJavaUtil</code> une requête de type <code>select</code> comme en CQL. On obtient ainsi un RDD de lignes Cassandra:
<pre class="java code java" style="font-family: inherit;">CassandraJavaRDD<span style="color: #339933;">&lt;</span>CassandraRow<span style="color: #339933;">&gt;</span> metricRdd <span style="color: #339933;">=</span> javaFunctions<span style="color: #009900;">(</span>sparkContext<span style="color: #009900;">)</span>         .<span style="color: #006633;">cassandraTable</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metrics"</span>, <span style="color: #0000ff;">"metric"</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">select</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"host"</span>, <span style="color: #0000ff;">"name"</span>, <span style="color: #0000ff;">"date"</span>, <span style="color: #0000ff;">"value"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Au passage, notez qu'en Scala, les conversions implicites auraient permis d'éviter l'appel à <code>javaFunctions</code>.

Le connecteur sait automatiser le mapping ligne/objet à la manière d'un ORM (voir <code>mapRowTo</code>), la <code>columnMap</code> contient la correspondance colonne/propriété:
<pre class="java code java" style="font-family: inherit;">CassandraJavaRDD<span style="color: #339933;">&lt;</span>Metric<span style="color: #339933;">&gt;</span> metricRdd <span style="color: #339933;">=</span> javaFunctions<span style="color: #009900;">(</span>sparkContext<span style="color: #009900;">)</span>         .<span style="color: #006633;">cassandraTable</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metrics"</span>, <span style="color: #0000ff;">"metric"</span>, mapRowTo<span style="color: #009900;">(</span>Metric.<span style="color: #000000; font-weight: bold;">class</span>, columnMap<span style="color: #009900;">)</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">select</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"host"</span>, <span style="color: #0000ff;">"name"</span>, <span style="color: #0000ff;">"date"</span>, <span style="color: #0000ff;">"value"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Le chemin inverse, l'écriture d'un RDD dans Cassandra, est très similaire à la lecture. La méthode <code>saveToCassandra</code> est une action Spark tout comme <code>saveAsTextFile</code>:
<pre class="java code java" style="font-family: inherit;">javaFunctions<span style="color: #009900;">(</span>metricStatRdd<span style="color: #009900;">)</span>         .<span style="color: #006633;">writerBuilder</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metrics"</span>, <span style="color: #0000ff;">"metric_stat"</span>, mapToRow<span style="color: #009900;">(</span>MetricStat.<span style="color: #000000; font-weight: bold;">class</span>, columnMap<span style="color: #009900;">)</span><span style="color: #009900;">)</span>         .<span style="color: #006633;">saveToCassandra</span><span style="color: #009900;">(</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
De manière à optimiser les accès base, il est de bon ton de placer les esclaves Spark sur les mêmes machines que les noeuds Cassandra.

<img style="display: block; margin: 0 auto;" title="Spark &amp; Cassandra Cluster" src="/wp-content/uploads/2015/07/spark-cassandra-cluster.png" alt="Spark &amp; Cassandra Cluster" />

Le connecteur Cassandra-Spark amène chaque exécutant Spark à préférer lire les partitions Cassandra du noeud sur lequel il est, évitant ainsi de coûteux allers-retours. Autrement dit, le connecteur sait optimiser les lectures, dès lors que Spark et Cassandra cohabitent sur un même noeud.
<h3>Spark SQL</h3>
Pour mémoire, Spark SQL est une extension de Spark qui permet d'exprimer les traitements sur un RDD dans un langage inspiré de SQL. Sous le capot, Spark SQL utilise un type de RDD spécialisé, nommé <code>SchemaRDD</code>, dans lequel les données seront structurées sous forme d'un tableau. Le <code>SchemaRDD</code> est constitué de lignes (<code>Row</code>), toutes ayant les mêmes colonnes. Pour passer d'un <code>RDD</code> tout simple à un <code>SchemaRDD</code>, on décrit les colonnes, puis on lui affecte un nom de table:
<pre class="java code java" style="font-family: inherit;"><span style="color: #666666; font-style: italic;">// Un RDD tout simple</span>     List<span style="color: #339933;">&lt;</span>Metric<span style="color: #339933;">&gt;</span> metrics <span style="color: #339933;">=</span> ...     <span style="color: #006633;">JavaRDD</span><span style="color: #339933;">&lt;</span>Metric<span style="color: #339933;">&gt;</span> simpleRdd <span style="color: #339933;">=</span> sparkContext.<span style="color: #006633;">parallelize</span><span style="color: #009900;">(</span>metrics<span style="color: #009900;">)</span><span style="color: #339933;">;</span>       <span style="color: #666666; font-style: italic;">// Un SchemaRDD</span>     JavaSQLContext sqlContext <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> JavaSQLContext(sparkContext<span style="color: #009900;">)</span><span style="color: #339933;">;</span>     JavaSchemaRDD schemaRDD <span style="color: #339933;">=</span> sqlContext.<span style="color: #006633;">applySchema</span><span style="color: #009900;">(</span>simpleRdd, Metric.<span style="color: #000000; font-weight: bold;">class</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span>     schemaRDD.<span style="color: #006633;">registerTempTable</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metric"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span>     JavaSchemaRDD resultRDD <span style="color: #339933;">=</span> sqlContext.<span style="color: #006633;">sql</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"select host, avg(value) from metric where name='cpu.total' group by host"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Une fois tables et colonnes déclarées dans le contexte, Spark SQL permet d'exécuter du pseudo-SQL sur n'importe quel RDD. En enregistrant plusieurs tables dans le context Spark SQL, on aurait pu faire faire des jointures entre elles.

Revenons à Cassandra, avec le connecteur Cassandra Spark, on peut utiliser Spark SQL pour requêter directement la base de données et produire des <code>SchemaRDD</code>s.
<pre class="java code java" style="font-family: inherit;">JavaCassandraSQLContext cassandraSQLContext <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> JavaCassandraSQLContext<span style="color: #009900;">(</span>sc<span style="color: #009900;">)</span><span style="color: #339933;">;</span>     JavaSchemaRDD metricRDD <span style="color: #339933;">=</span> cassandraSQLContext.<span style="color: #006633;">sql</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"select name from metrics.metric where name like 'cpu%'"</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span>     <span style="color: #000066; font-weight: bold;">long</span> count <span style="color: #339933;">=</span> metricRDD 	.<span style="color: #006633;">map</span><span style="color: #009900;">(</span>row <span style="color: #339933;">-&gt;</span> row.<span style="color: #006633;">getString</span><span style="color: #009900;">(</span><span style="color: #cc66cc;">0</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span> 	distinct<span style="color: #009900;">(</span><span style="color: #009900;">)</span>.<span style="color: #006633;">count</span><span style="color: #009900;">(</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
On peut aussi appliquer des transformations habituelles (map, filter), du cache, etc., sur le résultat de la requête comme sur tout RDD qui se respecte.

Pourquoi utiliser Spark SQL alors qu'on dispose déjà de CQL? Spark SQL permet de:
<ul>
	<li>De requêter les données même si le modèle de données ne si prête pas. En effet, en CQL, on ne pourra pas exprimer une requête si le modèle ne s'y prête pas.</li>
	<li>De faire des jointures et d'utiliser de nombreux opérateurs comme <code>like</code> ou <code>group by</code> dans les exemples ci-dessous</li>
</ul>
Bien que plus riche et permissive, l'approche Spark SQL est aussi beaucoup plus coûteuse que du CQL. En effet, Spark va généralement balayer un gros volume de données à la façon d'un full scan. Pour limiter l'impact de Spark sur les performances de Cassandra, il est vivement conseillé de créer un datacenter logique au niveau de Cassandra de manière à isoler cette charge.

Au final, Spark SQL ne remplace pas CQL, mais le complète pour tout ce qui est requêtes analytiques.
<h3>Spark Streaming</h3>
Si le coeur de Spark est focalisé sur les traitements en masse de gros volumes de données, Spark Streaming propose une API similaire pour des traitements de données au fil de l'eau. On troque les forts volumes de données pour de forts débits, et les RDD pour des <code>DStreams</code> (Discrete Streams). Mais l'API reste très proche, <code>map</code>, <code>filter</code> sont toujours de la partie:
<pre class="java code java" style="font-family: inherit;">JavaStreamingContext streamingContext <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> JavaStreamingContext<span style="color: #009900;">(</span>sparkContext, 	<span style="color: #000000; font-weight: bold;">new</span> Duration<span style="color: #009900;">(</span><span style="color: #cc66cc;">10000</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span>     JavaDStream<span style="color: #339933;">&lt;</span>Metric<span style="color: #339933;">&gt;</span> metricDStream <span style="color: #339933;">=</span> streamingContext 	    .<span style="color: #006633;">socketTextStream</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metric-source"</span>, <span style="color: #cc66cc;">7075</span><span style="color: #009900;">)</span> 	    .<span style="color: #006633;">map</span><span style="color: #009900;">(</span>Metric<span style="color: #339933;">::</span>parseStream<span style="color: #009900;">)</span> 	    .<span style="color: #006633;">filter</span><span style="color: #009900;">(</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">(</span><span style="color: #009900;">)</span> <span style="color: #339933;">!=</span> <span style="color: #000066; font-weight: bold;">null</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Ces DStreams seront alimentés par des systèmes orientés message/événement comme Kafka, ZeroMQ, Akka, etc. Cependant, les traitements seront effectués de manière périodique (toutes les 10 secondes dans l'exemple ci-dessus). Spark Streaming est un framework de <em>micro-batch</em>, il traite les données sous forme de petits RDDs toutes les N secondes (N entre 1 seconde et 1 minute):

<img style="display: block; margin: 0 auto;" title="Spark Streamin Microbatch" src="/wp-content/uploads/2015/07/spark-streaming-microbatch.png" alt="Spark Streamin Microbatch" />

Le fait d'avoir des RDDs ouvre pas mal de possibilités comme factoriser du code avec les traitements batch classiques, faire des jointures...

Et Cassandra dans tout ça? Et bien le connecteur permet de persister un <code>DStream</code> dans une table de la même manière qu'un RDD:
<pre class="java code java" style="font-family: inherit;">javaFunctions<span style="color: #009900;">(</span>metricDStream<span style="color: #009900;">)</span>                 .<span style="color: #006633;">writerBuilder</span><span style="color: #009900;">(</span><span style="color: #0000ff;">"metrics"</span>, <span style="color: #0000ff;">"metric"</span>, mapRowTo<span style="color: #009900;">(</span>Metric.<span style="color: #000000; font-weight: bold;">class</span>, columnMap<span style="color: #009900;">)</span><span style="color: #009900;">)</span>                 .<span style="color: #006633;">saveToCassandra</span><span style="color: #009900;">(</span><span style="color: #009900;">)</span><span style="color: #339933;">;</span></pre>
Cassandra pour le stockage et le requêtage temps réel, Spark pour le requêtage analytique, les traitements de masse et l'intégration au fil de l'eau. Ce redoutable tandem est capable d'absorber des données en grande quantité et procéder à des analyses de données à grande échelle. Spark apporte à Cassandra des capacités de requêtage et de fouille des données, Cassandra apporte à Spark une solution pour persister
les RDDs de manière distribuée et structurée.
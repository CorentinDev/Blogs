---
ID: 506
post_title: Introduction à Spark
author: Gérald Quintana
post_date: 2015-02-09 14:30:00
post_excerpt: |
  <p><img src="/public/spark-intro/spark-logo.png" alt="Logo Spark" style="float:right; margin: 0 0 1em 1em;" title="Logo Spark" /> <a href="http://spark.apache.org">Spark</a> est un framework pour traiter de gros volumes de données de manière distribuée. Initialement imaginé dans le laboratoire <a href="https://amplab.cs.berkeley.edu/software/">AMPLab</a> de l'université de Berkeley pour répondre à certains manques des frameworks Map/Reduce, c'est aujourd'hui un projet open-source hébergé par la fondation Apache, supporté par <a href="http://www.databricks.com">Databricks</a> et les principales distributions Hadoop (Cloudera, Hortonworks, MapR).</p>
layout: post
permalink: http://blog.zenika-offres.com/?p=506
published: true
slide_template:
  - ""
---
<p><img src="/wp-content/uploads/2015/07/spark-logo.png" alt="Logo Spark" style="float:right; margin: 0 0 1em 1em;" title="Logo Spark" /> <a href="http://spark.apache.org">Spark</a> est un framework pour traiter de gros volumes de données de manière distribuée. Initialement imaginé dans le laboratoire <a href="https://amplab.cs.berkeley.edu/software/">AMPLab</a> de l'université de Berkeley pour répondre à certains manques des frameworks Map/Reduce, c'est aujourd'hui un projet open-source hébergé par la fondation Apache, supporté par <a href="http://www.databricks.com">Databricks</a> et les principales distributions Hadoop (Cloudera, Hortonworks, MapR).</p>
<!--more-->
<h3>Des Streams aux RDDs</h3> <p>Si vous maîtrisez l'API Stream introduite dans Java 8, ou les <code>FluentIterable</code>s de Guava, vous ne serez pas déboussolés car Spark reprend les mêmes concepts:</p> <pre class="java code java" style="font-family:inherit"><span style="color: #666666; font-style: italic;">// Java 8</span>   metrics.<span style="color: #006633;">stream</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">filter</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">equals</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;cpu.user&quot;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">mapToLong</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">summaryStatistics</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">getAverage</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span> &nbsp;   <span style="color: #666666; font-style: italic;">// Spark</span>   metricRdd     .<span style="color: #006633;">filter</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">equals</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;cpu.user&quot;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">mapToDouble</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">mean</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span></pre> <p>On part d'un jeu de données sur lequel on applique en chaîne des opérations. Spark propose dans son DSL plus d'<a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations">une vingtaine d'opérateurs</a> dont <code>filter</code>, <code>map</code>, <code>flatMap</code>, <code>reduce</code> , <code>count</code>, <code>distinct</code>, <code>collect</code>, <code>forEach</code> comme dans les Streams Java 8 ou <code>join</code>, <code>leftOuterJoin</code>, <code>union</code> ou <code>groupBy</code> comme dans les bases de données relationnelles.</p> <p>Un RDD, ou <strong>Resilient Distributed Dataset</strong>, est le concept central du framework Spark. C'est un jeu de données qui se parcourt comme une collection. Il est distribué, car il sera vraisemblablement partitionné (découpé en partitions), et chacune des partitions traitée sur un noeud du cluster. Il est résilient, car il sera peut-être partiellement relu en cas de problème (perte d'un noeud par exemple).</p> <p>Comme une collection, un RDD peut-être constitué de types simples (Int, String...) ou structurés. Les types structurés (comme l'objet <code>Metric</code> dans l'exemple ci-dessus) seront immuables pour permettre la parallélisation et sérialisables car ils seront amenés à voyager d'un noeud à l'autre.</p> <p>Pour créer un RDD, on peut partir de:</p> <ul> <li><strong>Une collection</strong> (List, Set), transformée en RDD avec l'opérateur <code>parallelize</code></li> <li><strong>Un fichier</strong> local ou distribué (HDFS) dont le format est configurable: texte brut, SequenceFile Hadoop, JSON, ProtoBuf (via <a href="https://github.com/twitter/elephant-bird">Elephant Bird</a>)...</li> <li><strong>Une base de données</strong>: JDBC, Cassandra, HBase...</li> <li><strong>Un autre RDD</strong> auquel on aura appliqué une transformation comme <code>filter</code>, <code>map</code>...</li> </ul> <p>Le chemin inverse, exporter un RDD dans un fichier, dans une base de données ou un collection est aussi possible.</p> <h3>Du localisé au distribué</h3> <p>Lancer Spark en local est très utile pour développer gentiment sur son poste et tester unitairement ses chaînes de traitements. Il suffit de quelques lignes de code pour démarrer Spark:</p> <pre class="java code java" style="font-family:inherit"><span style="color: #000000; font-weight: bold;">public</span> <span style="color: #000000; font-weight: bold;">class</span> MetricSparkTest <span style="color: #009900;">&#123;</span>         <span style="color: #000000; font-weight: bold;">private</span> JavaSparkContext sparkContext<span style="color: #339933;">;</span>         @Before         <span style="color: #000000; font-weight: bold;">public</span> <span style="color: #000066; font-weight: bold;">void</span> setUp<span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span> <span style="color: #009900;">&#123;</span>             SparkConf conf <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> SparkConf<span style="color: #009900;">&#40;</span><span style="color: #000066; font-weight: bold;">true</span><span style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">setMaster</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;local&quot;</span><span style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">setAppName</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;Zenika&quot;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>             <span style="color: #000000; font-weight: bold;">this</span>.<span style="color: #006633;">sparkContext</span> <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> JavaSparkContext<span style="color: #009900;">&#40;</span>conf<span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>         <span style="color: #009900;">&#125;</span>         @Test         <span style="color: #000000; font-weight: bold;">public</span> <span style="color: #000066; font-weight: bold;">void</span> testMeanCpuUser<span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span> <span style="color: #000000; font-weight: bold;">throws</span> <span style="color: #003399;">Exception</span> <span style="color: #009900;">&#123;</span>             <span style="color: #003399;">Double</span> mean <span style="color: #339933;">=</span> sparkContext.<span style="color: #006633;">textFile</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;file://metric.txt&quot;</span><span style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">map</span><span style="color: #009900;">&#40;</span>Metric<span style="color: #339933;">::</span>parseLine<span style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">filter</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">equals</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;cpu.user&quot;</span><span style="color: #009900;">&#41;</span><sp
an style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">mapToDouble</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>                     .<span style="color: #006633;">mean</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>             assertEquals<span style="color: #009900;">&#40;</span>74.0D, mean, 0.1D<span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>         <span style="color: #009900;">&#125;</span>     <span style="color: #009900;">&#125;</span></pre> <p>Néanmoins, ne perdons pas de vue que l'objectif de Spark est de pouvoir répartir les traitements sur plusieurs machines. Dans ce but, une machine maître écoute les demandes de traitement clientes, découpe chaque traitement et, comme tout bon chef, délègue sa réalisation à des machines esclaves. Ces dernières vont accomplir le travail demandé en parallèle.</p> <p><img src="/wp-content/uploads/2015/07/spark-cluster.png" alt="Cluster Spark" /></p> <p>Pour passer d'une exécution locale à un cluster, seule la configuration Spark (voir l'objet <code>SparkConf</code>) change, l'emplacement du noeud maître n'est plus <code>local</code> mais <code>spark://...</code></p> <pre class="java code java" style="font-family:inherit">SparkConf conf <span style="color: #339933;">=</span> <span style="color: #000000; font-weight: bold;">new</span> SparkConf<span style="color: #009900;">&#40;</span><span style="color: #000066; font-weight: bold;">true</span><span style="color: #009900;">&#41;</span>             .<span style="color: #006633;">setMaster</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;spark://maitre:7077&quot;</span><span style="color: #009900;">&#41;</span></pre> <p>Pour paralléliser un traitement, il faut être capable de le découper. Ce partitionnement des données sera déterminé par la source de données ou bien pourra être imposé par le développeur. Premier exemple, pour HDFS on obtient par défaut une partition par bloc (64Mo):</p> <pre class="java code java" style="font-family:inherit">sparkContext.<span style="color: #006633;">textFile</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;hdfs://metric.txt&quot;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span></pre> <p>Second exemple, sur un RDD de type couple clé/valeur, on peut se servir de la clé (et d'une fonction de hachage) pour forcer le partitionnement:</p> <pre class="java code java" style="font-family:inherit">metricRdd     .<span style="color: #006633;">mapToPair</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> <span style="color: #000000; font-weight: bold;">new</span> Tuple2<span style="color: #339933;">&lt;&gt;</span><span style="color: #009900;">&#40;</span>metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>, metric<span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>     .<span style="color: #006633;">partitionBy</span><span style="color: #009900;">&#40;</span><span style="color: #000000; font-weight: bold;">new</span> HashPartitioner<span style="color: #009900;">&#40;</span><span style="color: #cc66cc;">100</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span></pre> <p>Sur des opérations comme les <code>join</code>, <code>groupBy</code> par exemple, la maîtrise du partitionnement est vital pour limiter le volume de données brassé entre les noeuds et ainsi garantir des performances acceptables.</p> <h3>Caching et lazy-evaluation</h3> <p>Revenons à l'exemple complet:</p> <pre class="java code java" style="font-family:inherit"><span style="color: #003399;">Double</span> mean <span style="color: #339933;">=</span> sparkContext.<span style="color: #006633;">textFile</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;file://metric.txt&quot;</span><span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 0</span>     .<span style="color: #006633;">map</span><span style="color: #009900;">&#40;</span>Metric<span style="color: #339933;">::</span>parseLine<span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 1</span>     .<span style="color: #006633;">filter</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span>  metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">equals</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;cpu.user&quot;</span><span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 2</span>     .<span style="color: #006633;">mapToDouble</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 3</span>     .<span style="color: #006633;">mean</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 4;</span></pre> <p>Lorsque les opérations 1 à 3 se déclenchent, rien ne se passe immédiatement, ce sont des transformations: elles convertissent un RDD en un autre RDD. En pratique, elles ne font qu'assembler des composants entre eux, chacun d'eux venant envelopper et décorer son ou ses prédécesseurs pour obtenir un nouveau RDD:</p> <p><img src="/wp-content/uploads/2015/07/spark-dag-1.png" alt="Spark DAG" /></p> <p>Seule la quatrième et dernière étape va réellement déclencher l'exécution de toutes les précédentes, c'est une action. Dans une architecture distribuée, cette action va se traduire par la soumission d'un job à l'ensemble du cluster.</p> <p>En procédant ainsi, pour effectuer un autre traitement sur le même jeu de données issu de l'étape 1, je serai amené à relire et reparser le fichier source (étapes 0 et 1). Avec les opérations <code>cache</code> et <code>persist</code>, Spark permet de conserver temporairement un résultat de transformation, cela permettra de réutiliser des résultats intermédiaires et d'éviter des recalculs superflus:</p> <pre class="java code java" style="font-family:inherit">JavaRDD<span style="color: #339933;">&lt;</span>Metric<span style="color: #339933;">&gt;</span> metricRdd <span style="color: #339933;">=</span> sparkContext.<span style="color: #006633;">textFile</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;file://metric.txt&quot;</span><span style="color: #009900;">&#41;</span>         .<span style="color: #006633;">map</span><span style="color: #009900;">&#40;</span>Metric<span style="color: #339933;">::</span>parse<span style="color: #009900;">&#41;</span>         .<span style="color: #006633;">cache</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>     <span style="color: #003399;">Double</span> mean <span style="color: #339933;">=</span> metricRdd         .<span style="color: #006633;">filter</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span>  metric.<span style="color: #006633;">getName</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span>.<span style="color: #006633;">equals</span><span style="color: #009900;">&#40;</span><span style="color: #0000ff;">&quot;cpu.use
r&quot;</span><span style="color: #009900;">&#41;</span> <span style="color: #666666; font-style: italic;">// 2</span>         .<span style="color: #006633;">mapToDouble</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>         .<span style="color: #006633;">mean</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span>     Map<span style="color: #339933;">&lt;</span>String, Long<span style="color: #339933;">&gt;</span> metricByHost <span style="color: #339933;">=</span> metricRdd         .<span style="color: #006633;">map</span><span style="color: #009900;">&#40;</span>metric <span style="color: #339933;">-&gt;</span> metric.<span style="color: #006633;">getHost</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #009900;">&#41;</span>         .<span style="color: #006633;">countByValue</span><span style="color: #009900;">&#40;</span><span style="color: #009900;">&#41;</span><span style="color: #339933;">;</span></pre> <p><img src="/wp-content/uploads/2015/07/spark-dag-2.png" alt="Cache Sparkj" /></p> <p>Spark permet ainsi de conserver temporairement un RDD en mémoire (on heap ou off heap avec <a href="http://tachyon-project.org/">Tachyon</a>) et/ou sur le disque local. Si le résultat n'a pas pu être conservé (cache miss), il sera recalculé.</p> <h3>Déploiement</h3> <p><a href="/wp-content/uploads/2015/07/spark-snapshot.png" title="Console Admin Spark"><img src="/wp-content/uploads/2015/07/.spark-snapshot_t.jpg" alt="Console Admin Spark" style="float:right; margin: 0 0 1em 1em;" title="Console Admin Spark" /></a> Exécuter des traitements lourds sur un cluster, piloter les noeud esclaves, leur distribuer les tâches équitablement, et arbitrer la quantité de CPU et de mémoire qui sera allouée à chacun des traitements, tel est le rôle d'un gestionnaire de cluster. Spark offre pour l'instant trois solutions pour cela: Spark standalone, YARN et Mesos. Livré avec Spark, Spark Standalone est le moyen le plus simple à mettre en place. Ce gestionnaire de cluster s'appuie sur Akka pour les échanges et sur Zookeeper pour garantir la haute-disponibilité du noeud maître. Ce n'est pas jouet, c'est un réel outil paré pour la production: Il dispose d'une console pour superviser les traitements, d'un mécanisme pour collecter les logs des esclaves...</p> <p>Pour lancer les process:</p> <pre>     // Spark Master     spark-class org.apache.spark.deploy.master.Master     // Spark Slave (Worker)     spark-class org.apache.spark.deploy.worker.Worker spark://maitre:7077 </pre> <p>Autre possibilité, YARN le gestionnaire de cluster Hadoop, Spark peut s'exécuter dessus, et aux côtés de jobs Hadoop. Enfin, plus sophistiqué et plus généraliste, Mesos permet de configurer plus finement l'allocation des ressources (mémoire, CPU) aux différentes applications.</p> <p>Une fois notre traitement mis au point et notre cluster prêt, il ne reste plus qu'à lancer le traitement. On commence par empaqueter tout le code et les librairies utilisées dans un gros Jar (un fatjar/uberjar). Puis on utilise la commande <code>spark-submit</code> pour soumettre le traitement au cluster:</p> <pre>     spark-submit --master spark://maitre:7077           --class com.zenika.metric.spark.Main            --executor-memory 1G                            --total-executor-cores 8                        metric-spark.jar </pre> <h3>Multi-langage</h3> <p>Les exemples présentés jusqu'ici étaient écrits en Java pour être accessibles au plus grand nombre. Cependant Spark est écrit en Scala et peut être indifféremment utilisé en Java, en Scala et en Python:</p> <ul> <li><strong>Scala</strong>: certaines fonctionnalités du langage comme les tuples, l'inférence de type, les case classes, les conversions implicites, rendent l'utilisation de Spark fluide. De plus, le Spark Shell, qui s'appuie sur le REPL Scala, permet l'écriture et l’exécution de traitements en direct.</li> <li><strong>Python</strong>: en attendant SparkR, c'est un des langages préférés des data scientists, on dispose, comme en Scala,  d'un REPL (PySpark), de tuples, d'un typage flexible... Mais certaines fonctionnalités purement Java ou Hadoop ne sont pas accessibles.</li> <li><strong>Java</strong>: la version 8 est presque obligatoire pour tirer parti des expressions lambda. La couche d'adaptation Java/Scala et le typage explicite nécessaire rend l'utilisation un peu plus lourde, mais reste acceptable.</li> </ul> <p>La plupart des exemples de la documentation sont décrits dans les 3 langages.</p> <h3>Modules additionnels</h3> <p>Au dessus de Spark, des librairies additionnelles apportent des fonctionnalités supplémentaires:</p> <ul> <li><strong>Spark SQL</strong>: permet d'exprimer les traitements (map, filter, reduce) sous la forme d'un langage inspiré de SQL</li> <li><strong>MLLib</strong>: est une bibliothèque d'algorithmes de Machine Learning pour classifier, regrouper les données  (k-Means), faire des recommandations...</li> <li><strong>Spark Stream</strong>: là où Spark excelle dans les traitements en masse de gros volumes de données, Spark Stream applique des recettes semblables pour des traitements au fil de l'eau</li> <li><strong>GraphX</strong>: apporte les outils pour explorer les graphes</li> </ul> <p>Dans cette courte introduction à Spark, nous avons parcouru les principaux concepts de ce framework de traitement distribué: RDD, transformations, actions, partitionnement... ainsi que quelques unes de ces qualités: développement simples et maintenables, possibilités d'optimisation... Je détaillerai un peu plus Spark SQL et Spark Stream dans <a href="index.php?post//2015/02/23/Spark-et-Cassandra">un prochain épisode</a>.</p> <p><a href="index.php?post/2015/02/23/Spark-et-Cassandra">A suivre...</a></p>